{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_categorizacion_blank.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agustin-sarasua/workshop-categorization/blob/master/workshop_categorizacion_blank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rU2-6XDJugy9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Workshop Categorizacion \n",
        "Items categorization with NLP, Keras, Tensorflow, Pandas and Numpy"
      ]
    },
    {
      "metadata": {
        "id": "x_TlFUAJuVFY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "from collections import Counter\n",
        "import random\n",
        "from IPython.display import Image\n",
        "# https://github.com/sepandhaghighi/pycm\n",
        "!pip install pycm==1.8\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,9))\n",
        "\n",
        "import pandas as pd # data preprocessing\n",
        "import numpy as np # linear algebra\n",
        "\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.preprocessing import text\n",
        "\n",
        "pd.set_option('display.max_columns', None)  # or 1000\n",
        "pd.set_option('display.max_rows', None)  # or 1000\n",
        "pd.set_option('display.max_colwidth', -1)  # or 199"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rT8HoVgQvCSL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gather the Data\n",
        "\n",
        "Download the zipped dataset of items from Google Drive (using the shareable link) and uncompress it."
      ]
    },
    {
      "metadata": {
        "id": "9apfpuWGuhGw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "download_file_from_google_drive('1kSPUjkf7gkB77UVTNnIW_cmGIjsa7GSh', 'arq.png')                \n",
        "download_file_from_google_drive('1lD7fMlzNhx0AaPkoksrqJ4HEiTL_u5KM', 'vec.png')\n",
        "\n",
        "download_file_from_google_drive('1Ql0az0GAC1_yHpHsAFvspKt1HZGMZOOa', 'cellphones.zip')\n",
        "!unzip 'cellphones.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFQE2kyovide",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is the architecture of the Neural Network that we are going to train"
      ]
    },
    {
      "metadata": {
        "id": "Z5IY2gbevas2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display(Image('arq.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HiI1NwEMvqTa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the dataset using pandas\n",
        "The dataset is a csv tab separated\n",
        "\n",
        "See: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.htm"
      ]
    },
    {
      "metadata": {
        "id": "Zt_KRTWHvlAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1 LoC aprox.\n",
        "def read_csv(path='cellphones.csv'):\n",
        "  dataset = # Your code here\n",
        "  return dataset\n",
        "\n",
        "df = read_csv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVC63MXdwQOb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Explore the Data"
      ]
    },
    {
      "metadata": {
        "id": "vajLlJ-fwMwt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df.describe()\n",
        "#df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwwWtFZPwtg4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Find how many classes we have in the dataset. \n",
        "\n",
        "This classes will be the output of the Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "P8PkZ-QkwYrJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1 LoC aprox.\n",
        "def get_num_classes(df):\n",
        "  output_length = # Your code here\n",
        "  return output_length\n",
        "\n",
        "num_classes = get_num_classes(df)\n",
        "print(\"Output Lenght:\", num_classes)\n",
        "print(\"# Training examples:\", len(df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V6wCQg2oxOBL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the # of items per class\n",
        "\n",
        "Bar plot the number of items per class.\n",
        "1 - before ploting sort de values by counts.\n",
        "\n",
        "See: \n",
        "Pandas Groupby https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\n",
        "\n",
        "Pandas Count https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.count.html\n",
        "\n",
        "Pandas Sort Values http://pandas.pydata.org/pandas-docs/version/0.19/generated/pandas.DataFrame.sort_values.html#pandas.DataFrame.sort_values\n",
        "\n",
        "Pandas plot bar https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.bar.html\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kcUCoj5xxlds",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# From 1 to 4 LoC aprox. \n",
        "def plot_items_per_class(df):\n",
        "  # Your code here\n",
        "\n",
        "plot_items_per_class(df)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MZn1Yzje32DJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the class distribution\n",
        "\n",
        "Plot the histogram - A histogram is a representation of the distribution of data\n",
        "\n",
        "See: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.hist.html"
      ]
    },
    {
      "metadata": {
        "id": "KnMZK8YS45xJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# From 1 to 4 LoC aprox. \n",
        "# Almost identical to previos solution\n",
        "def plot_distribution(df):\n",
        "  # Your code here\n",
        "\n",
        "plot_distribution(df)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R5FG1CgW6IiM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare the data for training\n",
        "Here is an image showing what we are to do with the data\n"
      ]
    },
    {
      "metadata": {
        "id": "vXS0_AF05MQo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display(Image('vec.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svzIj9yN6nOY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenization and Vectorization\n",
        "\n",
        "Tokenization: Divide the texts into words or smaller sub-texts (tokens). This determines the “vocabulary” of the dataset.\n",
        "\n",
        "Vectorization: Define a good numerical measure to characterize these texts.\n",
        "\n",
        "Steps:\n",
        "1. Fit a Keras Tokenizer using the text corpus (all the titles in the dataset)\n",
        "2. Build a dictionary of indexes -> tokens\n",
        "3. Create a function that parse the text to a sequence of decimals representing each token (step 2 in the image presented)\n",
        "4. Vectorize the text using different representations or techniques (tfidf, bow, binary, etc)\n",
        "5. Vectorize the labels (outputs of the NN)\n",
        "6. Build a dictionary of indexes -> labels\n"
      ]
    },
    {
      "metadata": {
        "id": "-imRqfH2_e6W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fit a Keras Tokenizer\n",
        "\n",
        "**From** https://keras.io/preprocessing/text/:\n",
        "\n",
        "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf..."
      ]
    },
    {
      "metadata": {
        "id": "E-XhMWcd6ZDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 2 LoC aprox.\n",
        "def fit_keras_tokenizer(corpus, num_features=5000):\n",
        "  tokenizer = # Your code here\n",
        "  # Your code here (remember to call fit_on_texts)\n",
        "  return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RblgcsIY_vo-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "tic = time.time()\n",
        "num_features=5000\n",
        "\n",
        "tokenizer = fit_keras_tokenizer(df['title'].values, num_features)\n",
        "\n",
        "toc = time.time()    \n",
        "print(\"Time to fit tokenizer: \" + str(1000*(toc-tic)) + \" ms\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6bjLJ3CFAEKs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build dictionary of index -> token\n",
        "\n",
        "It shoud return a dictionary with the following format: \n",
        "\n",
        "{..., 1: \"con\", 2: \"tablet\", ...}\n",
        "\n",
        "Note: use tokenizer.word_index"
      ]
    },
    {
      "metadata": {
        "id": "fnbqWClo_13S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1 LoC aprox. Could be more\n",
        "def build_idx_to_token(tokenizer):\n",
        "  idx_to_tkn = # Your code here\n",
        "  return idx_to_tkn\n",
        "\n",
        "idx_to_token = build_idx_to_token(tokenizer)\n",
        "print(idx_to_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bV7bhtryA-HN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parse titles to sequence of indexes\n",
        "This function returns a sequence of indices representing the text\n",
        "\n",
        "See https://keras.io/preprocessing/sequence/:\n",
        "\n",
        "sequence.pad_sequences\n",
        "\n",
        "use: tokenizer.texts_to_sequences(text)"
      ]
    },
    {
      "metadata": {
        "id": "VctTSXBcAXQS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "text: list of titles\n",
        "max_seq_length: fixed length to the titles. \n",
        "  If the title has more than \"max_seq_length\" tokens => truncate it to max_seq_length\n",
        "  If the title has less than \"max_seq_length\" tokens => complete at the begining with 0's \n",
        "'''\n",
        "# 2 LoC aprox.\n",
        "def parse_title_to_sequences(text, tokenizer, max_seq_length=12):\n",
        "  # 1 - text to sequences\n",
        "  text_sequences = # Your code here\n",
        "  # 2 - Pad the sequences to a fix length, so every example has the same length\n",
        "  text_sequences = # Your code here\n",
        "  return text_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KbJk_CAgEjzw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualize what the vectorizer is actually doing"
      ]
    },
    {
      "metadata": {
        "id": "7ypr9PK5B82b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_vectorized_title(title, tokenizer, idx_to_token, num_features, mode='count'):\n",
        "  print(\"------------------\")\n",
        "  print(\"Title:\", title)\n",
        "  print(\"------------------\")\n",
        "  print(\"Vectorizing title using %s mode\" % mode)\n",
        "  print(\"------------------\")\n",
        "  text_sequences = parse_title_to_sequences([title], tokenizer)\n",
        "\n",
        "  vectorized_title = tokenizer.sequences_to_matrix(text_sequences.tolist(), mode=mode)\n",
        "  assert vectorized_title.shape == (1, num_features)\n",
        "\n",
        "  for tkn in set(text_sequences[0]):\n",
        "    if tkn > 0:\n",
        "      print(idx_to_token[tkn] ,vectorized_title[0][tkn])\n",
        "\n",
        "title = \"tablet samsung de tablet con para pantalla tablet\"\n",
        "print_vectorized_title(title, tokenizer, idx_to_token, num_features, 'binary')\n",
        "print_vectorized_title(title, tokenizer, idx_to_token, num_features, 'count')\n",
        "print_vectorized_title(title, tokenizer, idx_to_token, num_features, 'tfidf')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3cmrezYdCt5N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Vectorize the text and labels"
      ]
    },
    {
      "metadata": {
        "id": "zZ0CDIdODapx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function append a new column called \"label\" with an integer that uniquely identify that domain_id.\n",
        "See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html\n"
      ]
    },
    {
      "metadata": {
        "id": "EEaMB94vCTr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1 LoC aprox\n",
        "def parse_labels_to_decimal(df):\n",
        "  df['label'] = # Your code here\n",
        "parse_labels_to_decimal(df)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n91lc4cSGx48",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This builds the labels vocabulary. It shoud return a dictionary with the following format: \n",
        "{..., 1: \"TABLETS\", 2: \"CELLPHONES\", ...}"
      ]
    },
    {
      "metadata": {
        "id": "ayHyaOmLGyp2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1 LoC aprox. Could be more\n",
        "def build_idx_to_label(df):\n",
        "  idx_to_label = # Your code here\n",
        "  return idx_to_label\n",
        "\n",
        "idx_to_label = build_idx_to_label(df)\n",
        "# Check that everything is consistent\n",
        "assert df[df['label']==3].reset_index().iloc[0]['domain_id'] == idx_to_label[3]\n",
        "print(idx_to_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIsh9qdZDvw5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will be using categorical_crossentropy loss function (softmax), for this reason, we need to use one_hot encoding for our labels.\n",
        "\n",
        "The representation should look like: [0, 0, ..., 1, 0] sparse vector with a \"1\" in the position of the target label.\n",
        "  \n",
        " See https://keras.io/utils/ (to_categorial function)"
      ]
    },
    {
      "metadata": {
        "id": "lAXDqg14DqEJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.utils import to_categorical\n",
        "\n",
        "# 1 LoC aprox\n",
        "def vectorize_labels(data, num_labels):\n",
        "  # Convert labels to categorical one-hot encoding\n",
        "  one_hot_labels = # Your code here\n",
        "  return one_hot_labels\n",
        "\n",
        "labels = vectorize_labels(df['label'].values, len(set(df['label'].values)))\n",
        "# Run some Checks \n",
        "assert df.iloc[10]['domain_id'] == idx_to_label[df.iloc[10]['label']]\n",
        "assert labels.shape[1] == num_classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gh-dQurRELvA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Split the dataset\n",
        "\n",
        "We will use 80% for training and 20% for validation\n",
        "\n",
        "See: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "metadata": {
        "id": "jOy8ONaoEMOK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1 LoC aprox.\n",
        "def split_and_shuffle_dataset(df, val_size=0.2):\n",
        "  # Random shuffle the dataset\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  df_train, df_val = # Your code here\n",
        "  return df_train.reset_index(drop=True), df_val.reset_index(drop=True)\n",
        "\n",
        "df_train, df_val = split_and_shuffle_dataset(df)\n",
        "print(df_train.shape)\n",
        "print(df_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TACn11RcGjE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vectorize the datasets "
      ]
    },
    {
      "metadata": {
        "id": "aMWU8rhQEhDb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vectorize_the_datasets(df, df_train, df_val, tokenizer, mode='tfidf'):\n",
        "  \n",
        "  x_train_seq = parse_title_to_sequences(df_train['title'].values, tokenizer)\n",
        "  x_train_vec = tokenizer.sequences_to_matrix(x_train_seq.tolist(), mode=mode)\n",
        "  y_train_vec = vectorize_labels(df_train['label'].values, len(df.domain_id.unique()))\n",
        "\n",
        "  x_val_seq = parse_title_to_sequences(df_val['title'].values, tokenizer)\n",
        "  x_val_vec = tokenizer.sequences_to_matrix(x_val_seq.tolist(), mode=mode)\n",
        "  y_val_vec = vectorize_labels(df_val['label'].values, len(df.domain_id.unique()))\n",
        "  \n",
        "  return x_train_vec, y_train_vec, x_val_vec, y_val_vec\n",
        "\n",
        "x_train, y_train, x_val, y_val = vectorize_the_datasets(df, df_train, df_val, tokenizer, 'tfidf')  \n",
        "\n",
        "# Run some checks\n",
        "index = 45\n",
        "assert y_train[index][df_train.iloc[index]['label']] == 1.\n",
        "assert y_val[index][df_val.iloc[index]['label']] == 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SImt3ligO7lA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Build the model\n",
        "We will first create a model using Keras Sequential API\n",
        "\n",
        "Learn More: https://keras.io/getting-started/sequential-model-guide/"
      ]
    },
    {
      "metadata": {
        "id": "rV2NdFoeHYTc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "\n",
        "def build_model(num_features, num_classes):\n",
        "  model = Sequential([\n",
        "      Dense(512, input_shape=(num_features,), activation='relu'),\n",
        "      Dense(num_classes, activation='softmax'),\n",
        "  ])\n",
        "\n",
        "  # For a multi-class classification problem\n",
        "  model.compile(optimizer=Adam(lr=0.0001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = build_model(x_train.shape[1], y_train.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuXHNIPzPqON",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing the Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "LkqtB3YrPAGU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt install graphviz;\n",
        "!pip install pydot pydot-ng;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ipBGsMENP5nU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "# Model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Plot model graph\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "display(Image('model.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5hdtUg6sQMfy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "It is time to train the model\n",
        "\n",
        "See: https://keras.io/models/model/ (fit function)"
      ]
    },
    {
      "metadata": {
        "id": "cl7MlpJtQMOM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def train_model(x_train, y_train, x_val, y_val):\n",
        "  # Create callback for early stopping on validation loss. If the loss does\n",
        "  # not decrease in two consecutive tries, stop training.\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
        "\n",
        "  # Train and validate model.\n",
        "  history = model.fit(\n",
        "      x_train,\n",
        "      y_train,\n",
        "      callbacks=callbacks,\n",
        "      epochs=20,\n",
        "      validation_data=(x_val, y_val),\n",
        "      verbose=1,  # Logs once per epoch.\n",
        "      batch_size=128)\n",
        "  return history\n",
        "\n",
        "history = train_model(x_train, y_train, x_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-E3k3X67SmZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the learning curves"
      ]
    },
    {
      "metadata": {
        "id": "gYs0ssjJP6eQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_training_history(history):\n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "visualize_training_history(history)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6fPqprqkS9FG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "metadata": {
        "id": "uWIAxB66v5NM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Write a function that takes any datasets (train or validation or any other), makes a prediction and return the decimal representation of the label\n",
        "\n",
        "See: model.predict function https://keras.io/models/model/\n",
        "\n",
        "See: np.argmax function https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html\n"
      ]
    },
    {
      "metadata": {
        "id": "q1sciRRySruZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 2 LoC aprox.\n",
        "def evaluate_model(x_data, model):\n",
        "  y_pred = # Your code here\n",
        "  y_pred = # Your code here\n",
        "  return y_pred\n",
        "\n",
        "y_pred = evaluate_model(x_val, model)\n",
        "y_act = np.argmax(y_val, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MOITzf-mwsKO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the errors and metrics"
      ]
    },
    {
      "metadata": {
        "id": "lueY4Lhyr-wF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# From https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
        "from sklearn.metrics import f1_score\n",
        "from pycm import *\n",
        "\n",
        "cm = ConfusionMatrix(actual_vector=y_act, predict_vector=y_pred)\n",
        "cm.save_html(\"cm_html\")\n",
        "#cm.print_matrix()\n",
        "#cm.print_normalized_matrix()\n",
        "#cm.print_matrix(one_vs_all=True, class_name=2)\n",
        "\n",
        "print(\"Global F1-Score:\" , f1_score(y_act, y_pred, average='micro'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ppYV7xGsTlYg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_errors(df_val, y_act, y_pred, idx_to_label, class_label=None, pred_label=None):\n",
        "  for idx, row in df_val.iterrows():\n",
        "    if class_label is not None and pred_label is not None:\n",
        "      if y_act[idx] != y_pred[idx] and row['label']==class_label and y_pred[idx] == pred_label:\n",
        "        print(row['item_id'], row['title'], 'Truth:', row['domain_id'], 'Prediction:',idx_to_label[y_pred[idx]])\n",
        "    else:\n",
        "      if y_act[idx] != y_pred[idx]:\n",
        "        print(row['item_id'], row['title'], 'Truth:', row['domain_id'], 'Prediction:',idx_to_label[y_pred[idx]])\n",
        "\n",
        "print_errors(df_val, y_act, y_pred, idx_to_label, 17, 8)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WC5bF8AKUHf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Y_act\", y_act[:10])\n",
        "print(\"Y_pred\",y_pred[:10])\n",
        "print(\"Title:\", df_val.iloc[0]['title'])\n",
        "assert df_val.iloc[1]['domain_id'] == idx_to_label[y_pred[1]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SirbIb_6zzbZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That is all for now! Thank you very much and be happy!"
      ]
    },
    {
      "metadata": {
        "id": "nO5bbA-80K_b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}